# ğŸ’¬ NLP Task 6 â€“ Question Answering with Transformers 

## ğŸš€ ELEVVO Internship | Abhisek Barik  
### ğŸŒŸ Level-3 â†’ NLP Task 6 âœ… + Bonus ğŸ’¡ Completed  

---

## ğŸ“Œ Task Overview  
Build a robust **Question Answering (QA) system** using transformer-based models, enabling the model to extract accurate answers from context passages. Evaluated on **Exact Match (EM)** and **F1 Score** using Stanford-style dataset.

---

## ğŸ§  Whatâ€™s Inside  
- âœ… Built a QA system using **DistilBERT**, **BERT**, **RoBERTa**, and **ALBERT**
- ğŸ“ˆ Evaluated on **custom + SQuAD-format** dataset (title, context, question, answer, answer_start, answer_end)
- ğŸ’¯ Achieved **90% Exact Match**, **95% Avg F1 Score**
- ğŸ§ª Model comparison with performance metrics
- ğŸŒ Deployed a **Streamlit interface** for live QA  

---

## ğŸ“Š Model Performance Summary  

| ğŸ” Model      | âš¡ Exact Match (%) | ğŸ¯ F1 Score (%) |
|--------------|-------------------|----------------|
| ğŸ† DistilBERT | 90.00             | 95.00          |
| BERT         | 90.00             | 92.67          |
| RoBERTa      | 80.00             | 93.00          |
| ALBERT       | 70.00             | 80.00          |

â¡ï¸ **RoBERTa** impressed with its high F1, though **DistilBERT** took the crown with both **speed** and **precision**  
â¡ï¸ **ALBERT** trades performance for lightweight efficiency â€” great for constrained devices  
â¡ï¸ **BERT** delivers a strong balance of accuracy and stability â€” a reliable baseline

---

## ğŸ§° Tech Stack  
**Python** ğŸ | **Hugging Face Transformers** ğŸ¤— | **PyTorch** âš™ï¸ | **pandas**, **NumPy**, **tqdm** ğŸ“Š  
**Evaluation**: Exact Match, F1 Score ğŸ¯ | **Streamlit** for QA App ğŸŒ  
**Dataset Format**: `title`, `context`, `question`, `answer`, `answer_start`, `answer_end`

---

## ğŸ”„ Workflow  

### âœ… 1. Data Handling  
- Parsed contextâ€“questionâ€“answer triplets  
- Calculated `answer_end` positions  
- Prepared Stanford-style QA JSON format

### ğŸ§  2. Model Implementation  
- Loaded **DistilBERT**, **BERT**, **RoBERTa**, and **ALBERT** from ğŸ¤— Transformers  
- Tokenized input for QA (context + question)  
- Extracted start and end logits to get answer spans

### ğŸ“ˆ 3. Evaluation  
- Custom function to compute **Exact Match** and **F1 Score**  
- Averaged across all models

### ğŸŒ 4. Streamlit App  
- Built real-time interface to input passage and question  
- Displayed extracted answers using selected models  

---

## ğŸš€ Bonus Features  

### ğŸ” Model Comparison Summary  
Tested all four models under the same dataset and evaluation criteria, revealing trade-offs between size, speed, and accuracy.

### ğŸ§ª Evaluation Insights  
- Normalized answers before matching  
- Highlighted text spans  
- Tracked answer position accuracy  

---

## ğŸ’¡ Learnings  
- Question Answering is an **extractive NLP task**, best tackled with fine-tuned transformers  
- Different transformer models have strengths: **speed**, **accuracy**, or **compactness**  
- Simple UI via Streamlit brings **hands-on interactivity** to QA pipelines  

---

## ğŸ“š Concepts Covered  
- âœ‚ï¸ Tokenization, Attention Masks  
- ğŸ” Context-Question Encoding  
- ğŸ§  Transformer Architecture (BERT family)  
- ğŸ“ Evaluation: EM & F1 Metrics  
- ğŸŒ Streamlit App Deployment  

---
